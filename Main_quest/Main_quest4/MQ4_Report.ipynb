{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY8EsLoKYA2UExmjFEfYC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWMV/AIFFEL_Quest1/blob/master/Main_quest/Main_quest4/MQ4_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MQ4. PNEUMONIA Classification Report & memoir(회고)\n",
        "\n",
        "# AIFFEL Online Core 6기 최현우\n",
        "\n",
        "## Flow Chart\n",
        "<이미지 추가>\n",
        "\n",
        "\n",
        "---\n",
        "## 1. Use model\n",
        "* ResNet18 model 구현\n",
        "* Risidual Learning : Skip Connection(identity_block) 기법 활용\n",
        "* Data input size : 180, 180, 3\n",
        "* Classification Class : NORMAL(정상), PNEUMONI(폐렴)\n",
        "\n",
        "## 2. Model Architecture\n",
        "* skip connection의 기법을 담은 'identity_block' function\n",
        "  * (X 이전레이어, f 필터크기, filters 채널 수)를 인자로 받고 X_shortcut = X로 Skip Connection을 정의 했습니다. 각 stage 마다 BottleNeck(1x1)로 연산량을 감소시키고, 다음 stage2,3은 이전 레이어의 필터를 받고 Conv를 진행한 후 final step에서 Skip connection과 이전 레이어를 합치고 출력 합니다\n",
        "\n",
        "* ResNet의 뼈대 Layer를 담은 'ResNet18' function\n",
        "  * input을 받고 7x7 층을 통과하여 사이즈를 줄이고, stage 별로 [64,128,256,512] 채널 수를 깊게하며 사이즈를 감소 시킵니다. stage 별로 Conv 층 수는 [3,4,6,5]개씩 구성하였습니다. 마지막 층에서 GlobalAvgPooling을 적용하고 FC 레이어를 통해 출력 하였습니다. 이진분류 task여서 출력 활성화함수는 'sigmoid'로 하였습니다\n",
        "\n",
        "  (참고 블로그 구조 이미지가 각 stage 별로 Conv층 수가 달랐습니다)\n",
        "<이미지 추가>\n",
        "\n",
        "---\n",
        "## 3. 성능 향상을 위한 기법\n",
        "* Data Augmentation (Flip, ZOOM, Rotation) : 데이터 수가 적음\n",
        "\n",
        "* Shuffle, repeat, batch, prefetch : 학습 성능을 향상 (prefetch 새로운 것 배움!)\n",
        "\n",
        "* Data balance 처리 : val data가 상대적으로 너무 적음\n",
        "\n",
        "* EarlyStopping : 에폭 수를 늘리면서 설정\n",
        "\n",
        "* learning_rate_scheduler : Local, global feature 마다 학습률 연관이 있다 하여 추가\n",
        "\n",
        "* GPU 사용 코드 적용 (새로운 것 배움)\n",
        "\n",
        "---\n",
        "## 4. 프로젝트 느낀점\n",
        "\n",
        "새로 알게 된 코드는 (기억!)으로 체크 추후 깊게 알아볼 예정이며, 5번 노드 코드를 기본으로 최종 프로젝트에서 추가한 부분은 '##'으로 체크하였습니다.\n",
        "\n",
        "지금까지 아이펠에서 해온 과정이 전부 녹아 있는 느낌이였습니다. 구조와 개념을 천천히 이어서 외워 가다보면 맞는 task에 활용할 수 있겠구나 느꼈습니다. 지금까지 한 공부가 빠질 것 없이 도움 되는 것에 퍼실님과 AIFFEL 과정에 감사합니다!\n",
        "\n",
        "어려웠던 점은 데이터를 분리 해주셔서 편하였지만 나중에 스스로 데이터를 수집하고 전처리 하는 과정을 잘 해낼 수 있을지 걱정 됩니다. 이런 부분도 한번 교육과정으로 배웠으면 하는 마음이 들었습니다.\n",
        "\n",
        "논문에 모델 구조 이미지를 보고 직접 Functional API 방식으로 구현 해보니 딥러닝을 한층 더 알게 된 기분이 들었습니다.\n",
        "\n",
        "---\n",
        "## 5. 참고 블로그\n",
        "(구조 이미지 참조)\n",
        "[참고 블로그](https://wjunsea.tistory.com/99)"
      ],
      "metadata": {
        "id": "eDX7awwl1GO9"
      }
    }
  ]
}